{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "gb-vpp-convlstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNGnvMlTqUvS1QlMbFZ8hdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavillan/gb-vpp/blob/main/models/gb-vpp-convlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_q1A_uRF_Jc",
        "outputId": "dc1e08d4-709f-447a-8f5c-e719d24c03d8"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pip install --upgrade kaggle > /dev/null 2>&1\n",
        "!mkdir -p ~/.kaggle/ && cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsJC-_8FViFK",
        "outputId": "3feb43f8-8a17-4b69-e16e-45b2499e4e68"
      },
      "source": [
        "!pip uninstall -y tensorflow \n",
        "!pip install tensorflow==2.4.3 > /dev/null 2>&1\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.6.0\n",
            "Uninstalling tensorflow-2.6.0:\n",
            "  Successfully uninstalled tensorflow-2.6.0\n",
            "Tensorflow version 2.4.3\n",
            "Running on TPU  ['10.91.136.138:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.91.136.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.91.136.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFa9wQ2IGrRR",
        "outputId": "aab7ef81-a364-4bba-a197-ef0e4121302a"
      },
      "source": [
        "!mkdir -p input/\n",
        "!kaggle competitions download -c ventilator-pressure-prediction -p input/ --force\n",
        "\n",
        "!unzip -o input/sample_submission.csv.zip -d input/\n",
        "!unzip -o input/train.csv.zip -d input/\n",
        "!unzip -o input/test.csv.zip -d input/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to input\n",
            " 93% 129M/139M [00:00<00:00, 159MB/s]\n",
            "100% 139M/139M [00:00<00:00, 159MB/s]\n",
            "Downloading sample_submission.csv.zip to input\n",
            "  0% 0.00/8.50M [00:00<?, ?B/s]\n",
            "100% 8.50M/8.50M [00:00<00:00, 78.0MB/s]\n",
            "Downloading test.csv.zip to input\n",
            " 86% 65.0M/75.4M [00:00<00:00, 166MB/s]\n",
            "100% 75.4M/75.4M [00:00<00:00, 153MB/s]\n",
            "Archive:  input/sample_submission.csv.zip\n",
            "  inflating: input/sample_submission.csv  \n",
            "Archive:  input/train.csv.zip\n",
            "  inflating: input/train.csv         \n",
            "Archive:  input/test.csv.zip\n",
            "  inflating: input/test.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hEWJqeFGonH"
      },
      "source": [
        "input_path = \"input\"\n",
        "subs_path = \"/content/drive/MyDrive/kaggle/gb-vpp/subs\"\n",
        "results_path = \"/content/drive/MyDrive/kaggle/gb-vpp/results\"\n",
        "artifacts_path = \"/content/drive/MyDrive/kaggle/gb-vpp/artifacts\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD2JeSsUJjyJ"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdrij3cGJi79"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "\n",
        "from IPython.display import display"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhdojM-wJ9HM"
      },
      "source": [
        "***\n",
        "## data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuScHGBrhkWT"
      },
      "source": [
        "SEQ_LEN = 80"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc0m-MxQJ02u"
      },
      "source": [
        "train_raw = pd.read_csv(f'{input_path}/train.csv')\n",
        "test_raw = pd.read_csv(f'{input_path}/test.csv')\n",
        "submission = pd.read_csv(f'{input_path}/sample_submission.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLMXEGnoPTCL"
      },
      "source": [
        "mapping = {j:i for i,j in enumerate(train_raw.breath_id.unique())}\n",
        "train_raw[\"breath_id\"] = train_raw.breath_id.map(mapping)\n",
        "\n",
        "if SEQ_LEN < 80:\n",
        "\n",
        "    train_raw = (\n",
        "        train_raw\n",
        "        .sort_values([\"breath_id\",\"time_step\"])\n",
        "        .groupby(\"breath_id\")\n",
        "        .head(SEQ_LEN)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    test_raw = (\n",
        "        test_raw\n",
        "        .sort_values([\"breath_id\",\"time_step\"])\n",
        "        .groupby(\"breath_id\")\n",
        "        .head(SEQ_LEN)\n",
        "        .reset_index(drop=True)\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izfe9H4mvCpn"
      },
      "source": [
        "def add_features(df):\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    \n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    \n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    \n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    \n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    df['cross']= df['u_in']*df['u_out']\n",
        "    df['cross2']= df['time_step']*df['u_out']\n",
        "    \n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    return df\n",
        "\n",
        "train = add_features(train_raw)\n",
        "test = add_features(test_raw)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr-VSykkKJoJ"
      },
      "source": [
        "# def compute_feats(dataframe):\n",
        "#     dataframe = dataframe.copy()\n",
        "\n",
        "#     # time features\n",
        "#     dataframe[\"time_diff\"] = dataframe.groupby(\"breath_id\")[\"time_step\"].diff()\n",
        "#     dataframe['time_since_expiratory']= dataframe['time_step'] * dataframe['u_out']\n",
        "\n",
        "#     # lag features\n",
        "#     lags = [1,2]\n",
        "#     for lag in lags:\n",
        "#         dataframe[f\"u_in_lag{lag}\"] = dataframe.groupby(\"breath_id\")[\"u_in\"].shift(lag).fillna(0)\n",
        "\n",
        "#     # stats on u_in\n",
        "#     dataframe[\"u_in_cumsum\"] = dataframe.groupby(\"breath_id\")[\"u_in\"].cumsum()\n",
        "#     dataframe['u_in_cummean'] =dataframe['u_in_cumsum'] / (dataframe.groupby(\"breath_id\")[\"id\"].cumcount()+1)\n",
        "#     #dataframe['u_in_last'] = dataframe.groupby('breath_id')['u_in'].transform('last')\n",
        "#     dataframe['cross1']= dataframe['u_in'] * dataframe['u_out']\n",
        "#     dataframe['cross2']= dataframe['u_in'] * (1 - dataframe['u_out'])\n",
        "\n",
        "#     dataframe['area'] = dataframe['time_step'] * dataframe['u_in']\n",
        "#     dataframe['area'] = dataframe.groupby('breath_id')['area'].cumsum()\n",
        "    \n",
        "#     dataframe['vol_diff'] = (dataframe['time_diff']*dataframe['u_in']).fillna(0)\n",
        "#     dataframe['vol_diff_cumsum'] = dataframe.groupby('breath_id')['vol_diff'].cumsum()\n",
        "\n",
        "#     # gradients of u_in\n",
        "#     def compute_grad_1st(df):\n",
        "#         return np.gradient(df.u_in, 100*df.time_step)\n",
        "#     def compute_grad_2nd(df):\n",
        "#         return np.gradient(df.grad_1st, 100*df.time_step)\n",
        "\n",
        "#     gb_result = dataframe.groupby(\"breath_id\").apply(compute_grad_1st)\n",
        "#     dataframe[\"grad_1st\"] = np.concatenate(gb_result.values)\n",
        "#     gb_result = dataframe.groupby(\"breath_id\").apply(compute_grad_2nd)\n",
        "#     dataframe[\"grad_2nd\"] = np.concatenate(gb_result.values)\n",
        "\n",
        "#     # nan filling\n",
        "#     dataframe[\"time_diff\"] = dataframe[\"time_diff\"].fillna(method=\"bfill\")\n",
        "    \n",
        "#     # ohe of R&C values    \n",
        "#     dataframe['R'] = dataframe['R'].astype(str)\n",
        "#     dataframe['C'] = dataframe['C'].astype(str)\n",
        "#     dataframe['RC'] = dataframe['R']+dataframe['C']\n",
        "#     dataframe = pd.get_dummies(dataframe)\n",
        "#     return dataframe\n",
        "\n",
        "# train = compute_feats(train_raw)\n",
        "# test  = compute_feats(test_raw)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FydxTC2zKb3r"
      },
      "source": [
        "targets = train[['pressure']].to_numpy().reshape(-1, SEQ_LEN)\n",
        "\n",
        "cols_to_exclude_train = exclude = [\"id\",\"breath_id\",\"pressure\"]\n",
        "cols_to_exclude_test = exclude = [\"id\",\"breath_id\",]\n",
        "\n",
        "train.drop(cols_to_exclude_train, axis=1, inplace=True)\n",
        "test.drop(cols_to_exclude_test, axis=1, inplace=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmiLTm8gK1eG"
      },
      "source": [
        "scaler = RobustScaler()\n",
        "train = scaler.fit_transform(train)\n",
        "test = scaler.transform(test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkjljVxeLIsu"
      },
      "source": [
        "train = train.reshape(-1, SEQ_LEN, train.shape[-1])\n",
        "test = test.reshape(-1, SEQ_LEN, train.shape[-1])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7-mHNoyLkNR"
      },
      "source": [
        "***\n",
        "## model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID91a7AzvgPQ"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzUU2cq3EUXc"
      },
      "source": [
        "def build_model(input_dim):\n",
        "    with tpu_strategy.scope():   \n",
        "        inputs = layers.Input(shape = input_dim)\n",
        "    \n",
        "        h1 = layers.Bidirectional(layers.LSTM(1024, return_sequences=True))(inputs)\n",
        "        h1 = tf.reshape(h1, shape=(-1,80,128,1,16))\n",
        "\n",
        "        h2 = layers.ConvLSTM2D(filters=256, kernel_size=(1,5), padding=\"same\", return_sequences=True, data_format=\"channels_first\")(h1)\n",
        "        h3 = layers.ConvLSTM2D(filters=128, kernel_size=(1,5), padding=\"same\", return_sequences=True, data_format=\"channels_first\")(h2)\n",
        "        h4 = layers.ConvLSTM2D(filters=64, kernel_size=(1,5), padding=\"same\", return_sequences=True, data_format=\"channels_first\")(h3)\n",
        "\n",
        "        h4 = tf.reshape(h4, shape=(-1,80,1024))\n",
        "\n",
        "\n",
        "\n",
        "        #h4 = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(h3)\n",
        "        #h5 = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(h4)\n",
        "        #h6 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(h5) \n",
        "                \n",
        "        #hcat = layers.Concatenate()([h4,h6])\n",
        "        #hcat = layers.Dropout(0.1)(hcat)\n",
        "\n",
        "        out = layers.Dense(128, activation = 'selu')(h4)\n",
        "        out = layers.Dense(1)(out)\n",
        "        \n",
        "        model = keras.Model(inputs, out)\n",
        "        model.compile(optimizer=\"adam\", loss=\"mae\")\n",
        "           \n",
        "    return model  "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "lbXVT9UN7YIH",
        "outputId": "bbc81794-56ac-4364-e727-af3ecd96114a"
      },
      "source": [
        "model = build_model(input_dim=train.shape[-2:])\n",
        "display(model.summary())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 80, 50)]          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_11 (Bidirectio (None, 80, 2048)          8806400   \n",
            "_________________________________________________________________\n",
            "tf.reshape_18 (TFOpLambda)   (None, 80, 128, 1, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_30 (ConvLSTM2D) (None, 80, 256, 1, 16)    1967104   \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_31 (ConvLSTM2D) (None, 80, 128, 1, 16)    983552    \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_32 (ConvLSTM2D) (None, 80, 64, 1, 16)     246016    \n",
            "_________________________________________________________________\n",
            "tf.reshape_19 (TFOpLambda)   (None, 80, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 80, 128)           131200    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 80, 1)             129       \n",
            "=================================================================\n",
            "Total params: 12,134,401\n",
            "Trainable params: 12,134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbhtf4DM1JaY",
        "outputId": "e9ae2243-9655-48a8-99e2-1c0d46c4ee09"
      },
      "source": [
        "EPOCH = 300\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "models_by_fold = list()\n",
        "\n",
        "oof = train_raw[[\"id\",\"breath_id\",\"u_out\",\"pressure\"]].copy()\n",
        "\n",
        "for fold, (train_idx,valid_idx) in enumerate(kf.split(train, targets)):\n",
        "\n",
        "    print(f\" Fold: {fold+1} \".center(80, \"-\"))\n",
        "    X_train, X_valid = train[train_idx], train[valid_idx]\n",
        "    y_train, y_valid = targets[train_idx], targets[valid_idx]\n",
        "    \n",
        "    model = build_model(input_dim=train.shape[-2:])\n",
        "    display(model.summary())\n",
        "\n",
        "    #scheduler = ExponentialDecay(\n",
        "    #    initial_learning_rate=1e-3, \n",
        "    #    decay_steps=EPOCH*((len(train)*0.8)/BATCH_SIZE), \n",
        "    #    decay_rate=1e-5\n",
        "    #)\n",
        "    #lr = LearningRateScheduler(scheduler, verbose=1)\n",
        "    #lr = OneCycleScheduler(\n",
        "    #    lr_max = 1e-3,\n",
        "    #    steps = EPOCH*(X_train.shape[0]/BATCH_SIZE),\n",
        "    #    phase_1_pct = 0.2,\n",
        "    #    init_div_factor = 1e1,\n",
        "    #    final_div_factor = 1e2,\n",
        "    #)\n",
        "    lr = ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", \n",
        "        factor=0.8, \n",
        "        patience=10, \n",
        "        verbose=1\n",
        "    )\n",
        "    es = EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        mode='min', \n",
        "        patience=35, \n",
        "        verbose=1,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "    history = model.fit(\n",
        "        X_train, \n",
        "        y_train, \n",
        "        validation_data=(X_valid, y_valid), \n",
        "        epochs=EPOCH, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        callbacks=[lr,es],\n",
        "        verbose=1,\n",
        "    )\n",
        "    models_by_fold.append(model)\n",
        "\n",
        "    # generate the oof predictions\n",
        "    x_valid_tf = tf.convert_to_tensor(X_valid, dtype=tf.float32)\n",
        "    oof_preds = model.call(x_valid_tf, training=False).numpy().squeeze()\n",
        "    idx = oof.query(\"breath_id in @valid_idx\").index\n",
        "    oof.loc[idx, \"pred\"] = oof_preds.ravel()\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.plot(history.history[\"loss\"], \"o--\", label=\"loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], \"o--\", label=\"val_loss\")\n",
        "    plt.grid()\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.plot(history.history[\"loss\"][50:], \"o--\", label=\"loss\")\n",
        "    plt.plot(history.history[\"val_loss\"][50:], \"o--\", label=\"val_loss\")\n",
        "    plt.grid()\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------- Fold: 1 ------------------------------------\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        [(None, 80, 50)]          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_12 (Bidirectio (None, 80, 2048)          8806400   \n",
            "_________________________________________________________________\n",
            "tf.reshape_20 (TFOpLambda)   (None, 80, 128, 1, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_33 (ConvLSTM2D) (None, 80, 256, 1, 16)    1967104   \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_34 (ConvLSTM2D) (None, 80, 128, 1, 16)    983552    \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_35 (ConvLSTM2D) (None, 80, 64, 1, 16)     246016    \n",
            "_________________________________________________________________\n",
            "tf.reshape_21 (TFOpLambda)   (None, 80, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 80, 128)           131200    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 80, 1)             129       \n",
            "=================================================================\n",
            "Total params: 12,134,401\n",
            "Trainable params: 12,134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "59/59 [==============================] - 63s 717ms/step - loss: 4.5329 - val_loss: 1.6338\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 1.4462 - val_loss: 1.0914\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 1.0356 - val_loss: 0.8754\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.8830 - val_loss: 0.8436\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.8056 - val_loss: 0.7249\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.7152 - val_loss: 0.5849\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.5856 - val_loss: 0.5354\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.5100 - val_loss: 0.5064\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.4887 - val_loss: 0.4377\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4690 - val_loss: 0.4258\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4240 - val_loss: 0.4027\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.4120 - val_loss: 0.3968\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4099 - val_loss: 0.4067\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3966 - val_loss: 0.4049\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.3867 - val_loss: 0.3733\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.3740 - val_loss: 0.3658\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3631 - val_loss: 0.3485\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3477 - val_loss: 0.3617\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3561 - val_loss: 0.3353\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3376 - val_loss: 0.3423\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3344 - val_loss: 0.3399\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3279 - val_loss: 0.3547\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.3382 - val_loss: 0.3316\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 23s 386ms/step - loss: 0.3098 - val_loss: 0.3201\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3199 - val_loss: 0.3274\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3257 - val_loss: 0.3317\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3103 - val_loss: 0.3097\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3107 - val_loss: 0.3011\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2937 - val_loss: 0.2988\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2905 - val_loss: 0.3100\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2944 - val_loss: 0.2993\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2864 - val_loss: 0.3201\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2940 - val_loss: 0.3066\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2793 - val_loss: 0.2854\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2846 - val_loss: 0.2884\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2779 - val_loss: 0.2861\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2713 - val_loss: 0.2939\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2694 - val_loss: 0.2779\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2714 - val_loss: 0.2908\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2655 - val_loss: 0.3061\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2754 - val_loss: 0.2810\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2639 - val_loss: 0.2653\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2588 - val_loss: 0.2615\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2477 - val_loss: 0.2672\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2538 - val_loss: 0.2554\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2440 - val_loss: 0.2555\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2416 - val_loss: 0.2555\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2443 - val_loss: 0.2658\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2517 - val_loss: 0.2602\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2486 - val_loss: 0.2647\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2509 - val_loss: 0.2611\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2379 - val_loss: 0.2603\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2503 - val_loss: 0.2599\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2420 - val_loss: 0.2591\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2325 - val_loss: 0.2499\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2278 - val_loss: 0.2535\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2260 - val_loss: 0.2749\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2511 - val_loss: 0.2844\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2501 - val_loss: 0.2390\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2249 - val_loss: 0.2365\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2259 - val_loss: 0.2331\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2165 - val_loss: 0.2288\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2125 - val_loss: 0.2478\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2160 - val_loss: 0.2442\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2096 - val_loss: 0.2303\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2145 - val_loss: 0.2311\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.2121 - val_loss: 0.2358\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2166 - val_loss: 0.2343\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2287 - val_loss: 0.2309\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 22s 380ms/step - loss: 0.2090 - val_loss: 0.2300\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2084 - val_loss: 0.2351\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2044 - val_loss: 0.2204\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1988 - val_loss: 0.2225\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.2041 - val_loss: 0.2421\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2170 - val_loss: 0.2277\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 23s 385ms/step - loss: 0.2116 - val_loss: 0.2728\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2342 - val_loss: 0.2474\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2079 - val_loss: 0.2240\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1954 - val_loss: 0.2337\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2040 - val_loss: 0.2244\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.2019 - val_loss: 0.2272\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2020 - val_loss: 0.2387\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1988 - val_loss: 0.2121\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1790 - val_loss: 0.2041\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1753 - val_loss: 0.2019\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1724 - val_loss: 0.2032\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1710 - val_loss: 0.2027\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1677 - val_loss: 0.1987\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1735 - val_loss: 0.2081\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1707 - val_loss: 0.2059\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1681 - val_loss: 0.1999\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1719 - val_loss: 0.2000\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1654 - val_loss: 0.1969\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1668 - val_loss: 0.1987\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1665 - val_loss: 0.1997\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1659 - val_loss: 0.1962\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1612 - val_loss: 0.2049\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1750 - val_loss: 0.2188\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1827 - val_loss: 0.2087\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1734 - val_loss: 0.2093\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1706 - val_loss: 0.2103\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1708 - val_loss: 0.2129\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1713 - val_loss: 0.2087\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1665 - val_loss: 0.2016\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1646 - val_loss: 0.2350\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1827 - val_loss: 0.2125\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1669 - val_loss: 0.1991\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1501 - val_loss: 0.1934\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1519 - val_loss: 0.1987\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1545 - val_loss: 0.2054\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 23s 386ms/step - loss: 0.1565 - val_loss: 0.2056\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1532 - val_loss: 0.1897\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1481 - val_loss: 0.1969\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1502 - val_loss: 0.1976\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1466 - val_loss: 0.1920\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1479 - val_loss: 0.1974\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1482 - val_loss: 0.1890\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1475 - val_loss: 0.1966\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1441 - val_loss: 0.1914\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1420 - val_loss: 0.1882\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1422 - val_loss: 0.1896\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1411 - val_loss: 0.1880\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1412 - val_loss: 0.1892\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1386 - val_loss: 0.1899\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1382 - val_loss: 0.1888\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1341 - val_loss: 0.1867\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1360 - val_loss: 0.1948\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1356 - val_loss: 0.1911\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1372 - val_loss: 0.1885\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1356 - val_loss: 0.1868\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1319 - val_loss: 0.1870\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1313 - val_loss: 0.1878\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1349 - val_loss: 0.1859\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1303 - val_loss: 0.1868\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1297 - val_loss: 0.1863\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1314 - val_loss: 0.1869\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1316 - val_loss: 0.1890\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1325 - val_loss: 0.1901\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1320 - val_loss: 0.1910\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1304 - val_loss: 0.1954\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1288 - val_loss: 0.1854\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1257 - val_loss: 0.1864\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1273 - val_loss: 0.1936\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1284 - val_loss: 0.1928\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1284 - val_loss: 0.1870\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1255 - val_loss: 0.1909\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1282 - val_loss: 0.1853\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1240 - val_loss: 0.1882\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1277 - val_loss: 0.1870\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1236 - val_loss: 0.1880\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1255 - val_loss: 0.1873\n",
            "\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1162 - val_loss: 0.1823\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1114 - val_loss: 0.1859\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 23s 386ms/step - loss: 0.1119 - val_loss: 0.1826\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1125 - val_loss: 0.1833\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1115 - val_loss: 0.1846\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1129 - val_loss: 0.1868\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1133 - val_loss: 0.1840\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1104 - val_loss: 0.1849\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1113 - val_loss: 0.1825\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1093 - val_loss: 0.1846\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1106 - val_loss: 0.1866\n",
            "\n",
            "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1063 - val_loss: 0.1845\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1036 - val_loss: 0.1819\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1019 - val_loss: 0.1832\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1027 - val_loss: 0.1829\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1013 - val_loss: 0.1811\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1003 - val_loss: 0.1845\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1025 - val_loss: 0.1834\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1018 - val_loss: 0.1824\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1004 - val_loss: 0.1815\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1013 - val_loss: 0.1835\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1014 - val_loss: 0.1836\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0997 - val_loss: 0.1822\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0995 - val_loss: 0.1840\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0998 - val_loss: 0.1820\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0989 - val_loss: 0.1838\n",
            "\n",
            "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0955 - val_loss: 0.1821\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0912 - val_loss: 0.1815\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0908 - val_loss: 0.1819\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0925 - val_loss: 0.1811\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0918 - val_loss: 0.1818\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0913 - val_loss: 0.1826\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0904 - val_loss: 0.1834\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0903 - val_loss: 0.1824\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0910 - val_loss: 0.1826\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0907 - val_loss: 0.1828\n",
            "\n",
            "Epoch 00187: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0871 - val_loss: 0.1813\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0855 - val_loss: 0.1810\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0848 - val_loss: 0.1813\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0839 - val_loss: 0.1822\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 23s 387ms/step - loss: 0.0845 - val_loss: 0.1815\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0836 - val_loss: 0.1819\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0847 - val_loss: 0.1831\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0852 - val_loss: 0.1820\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0840 - val_loss: 0.1825\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0831 - val_loss: 0.1825\n",
            "\n",
            "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0811 - val_loss: 0.1809\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0793 - val_loss: 0.1816\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0791 - val_loss: 0.1818\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0788 - val_loss: 0.1813\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0785 - val_loss: 0.1815\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0797 - val_loss: 0.1818\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0787 - val_loss: 0.1826\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0783 - val_loss: 0.1820\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0777 - val_loss: 0.1825\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0784 - val_loss: 0.1831\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0783 - val_loss: 0.1826\n",
            "\n",
            "Epoch 00208: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0766 - val_loss: 0.1818\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0740 - val_loss: 0.1816\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0740 - val_loss: 0.1819\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0737 - val_loss: 0.1822\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0739 - val_loss: 0.1818\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.0731 - val_loss: 0.1822\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0733 - val_loss: 0.1821\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0734 - val_loss: 0.1825\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 22s 380ms/step - loss: 0.0729 - val_loss: 0.1823\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0731 - val_loss: 0.1822\n",
            "\n",
            "Epoch 00218: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0716 - val_loss: 0.1821\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0707 - val_loss: 0.1822\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0703 - val_loss: 0.1826\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0707 - val_loss: 0.1823\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0698 - val_loss: 0.1826\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0693 - val_loss: 0.1826\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0696 - val_loss: 0.1826\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0694 - val_loss: 0.1830\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0695 - val_loss: 0.1828\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0687 - val_loss: 0.1827\n",
            "\n",
            "Epoch 00228: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0684 - val_loss: 0.1827\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0672 - val_loss: 0.1827\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0672 - val_loss: 0.1829\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0670 - val_loss: 0.1829\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0665 - val_loss: 0.1829\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00233: early stopping\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------- Fold: 2 ------------------------------------\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 80, 50)]          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_13 (Bidirectio (None, 80, 2048)          8806400   \n",
            "_________________________________________________________________\n",
            "tf.reshape_22 (TFOpLambda)   (None, 80, 128, 1, 16)    0         \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_36 (ConvLSTM2D) (None, 80, 256, 1, 16)    1967104   \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_37 (ConvLSTM2D) (None, 80, 128, 1, 16)    983552    \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_38 (ConvLSTM2D) (None, 80, 64, 1, 16)     246016    \n",
            "_________________________________________________________________\n",
            "tf.reshape_23 (TFOpLambda)   (None, 80, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 80, 128)           131200    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 80, 1)             129       \n",
            "=================================================================\n",
            "Total params: 12,134,401\n",
            "Trainable params: 12,134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "59/59 [==============================] - 63s 697ms/step - loss: 4.6270 - val_loss: 1.6151\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 1.5192 - val_loss: 1.1291\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 1.0811 - val_loss: 1.0346\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.9454 - val_loss: 0.8277\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.8102 - val_loss: 0.7672\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.7354 - val_loss: 0.7021\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.6619 - val_loss: 0.5849\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.5648 - val_loss: 0.5685\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.5159 - val_loss: 0.4700\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4670 - val_loss: 0.4660\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4587 - val_loss: 0.5378\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4456 - val_loss: 0.4179\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3971 - val_loss: 0.4104\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.4151 - val_loss: 0.4016\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.3725 - val_loss: 0.4080\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3681 - val_loss: 0.3939\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.3530 - val_loss: 0.3679\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3533 - val_loss: 0.3557\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3532 - val_loss: 0.3699\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3402 - val_loss: 0.4296\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3665 - val_loss: 0.3379\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3316 - val_loss: 0.3283\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3257 - val_loss: 0.3241\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3062 - val_loss: 0.3367\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3077 - val_loss: 0.3277\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3015 - val_loss: 0.3400\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2944 - val_loss: 0.3200\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2998 - val_loss: 0.3066\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.3092 - val_loss: 0.3131\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2929 - val_loss: 0.3097\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2842 - val_loss: 0.3227\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3480 - val_loss: 0.3442\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.3063 - val_loss: 0.2971\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2783 - val_loss: 0.3156\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2743 - val_loss: 0.2930\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2753 - val_loss: 0.2842\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2582 - val_loss: 0.2740\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2622 - val_loss: 0.2660\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2505 - val_loss: 0.2794\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2535 - val_loss: 0.2812\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2517 - val_loss: 0.2883\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2520 - val_loss: 0.2725\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2427 - val_loss: 0.3041\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2601 - val_loss: 0.2648\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2419 - val_loss: 0.2635\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2465 - val_loss: 0.2719\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2327 - val_loss: 0.2697\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2356 - val_loss: 0.2564\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2335 - val_loss: 0.2437\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2326 - val_loss: 0.2486\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.2224 - val_loss: 0.2625\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2274 - val_loss: 0.2478\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2283 - val_loss: 0.2475\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2224 - val_loss: 0.2395\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2246 - val_loss: 0.2817\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2296 - val_loss: 0.2523\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 23s 392ms/step - loss: 0.2289 - val_loss: 0.2498\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2355 - val_loss: 0.2433\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2117 - val_loss: 0.2500\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2163 - val_loss: 0.2466\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2101 - val_loss: 0.2329\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2048 - val_loss: 0.2491\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2072 - val_loss: 0.2285\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2000 - val_loss: 0.2246\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1971 - val_loss: 0.2473\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.2120 - val_loss: 0.2265\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2009 - val_loss: 0.2436\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2001 - val_loss: 0.2278\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1950 - val_loss: 0.2536\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1969 - val_loss: 0.2242\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1947 - val_loss: 0.2424\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1944 - val_loss: 0.2228\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1896 - val_loss: 0.2223\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1961 - val_loss: 0.2384\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1885 - val_loss: 0.2151\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1821 - val_loss: 0.2265\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1872 - val_loss: 0.2326\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1931 - val_loss: 0.2415\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.2059 - val_loss: 0.2276\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1915 - val_loss: 0.2261\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1878 - val_loss: 0.2216\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1883 - val_loss: 0.2194\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1847 - val_loss: 0.2331\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1894 - val_loss: 0.2182\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1791 - val_loss: 0.2352\n",
            "\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1751 - val_loss: 0.2072\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1594 - val_loss: 0.2037\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1603 - val_loss: 0.2200\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1676 - val_loss: 0.2080\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1594 - val_loss: 0.2081\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1637 - val_loss: 0.2065\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1589 - val_loss: 0.2193\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1650 - val_loss: 0.2089\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1555 - val_loss: 0.2059\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1602 - val_loss: 0.2026\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1553 - val_loss: 0.2092\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1535 - val_loss: 0.2029\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1538 - val_loss: 0.2046\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1520 - val_loss: 0.2099\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1562 - val_loss: 0.2073\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1535 - val_loss: 0.2115\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1537 - val_loss: 0.2064\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1524 - val_loss: 0.2011\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1509 - val_loss: 0.2138\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1566 - val_loss: 0.2129\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1608 - val_loss: 0.2044\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1498 - val_loss: 0.2195\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 23s 390ms/step - loss: 0.1576 - val_loss: 0.2072\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1493 - val_loss: 0.2117\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1462 - val_loss: 0.2010\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1522 - val_loss: 0.2059\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1423 - val_loss: 0.2026\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1465 - val_loss: 0.2108\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1461 - val_loss: 0.2085\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1432 - val_loss: 0.1972\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1404 - val_loss: 0.2034\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1431 - val_loss: 0.2004\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1444 - val_loss: 0.1995\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1364 - val_loss: 0.1982\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.1376 - val_loss: 0.2018\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1378 - val_loss: 0.1999\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1389 - val_loss: 0.1976\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1387 - val_loss: 0.2016\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1370 - val_loss: 0.1992\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1361 - val_loss: 0.2022\n",
            "\n",
            "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1303 - val_loss: 0.2006\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1266 - val_loss: 0.1935\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1213 - val_loss: 0.1960\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1225 - val_loss: 0.1972\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1241 - val_loss: 0.2016\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1217 - val_loss: 0.1979\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1202 - val_loss: 0.1948\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1225 - val_loss: 0.1963\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1193 - val_loss: 0.1965\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1212 - val_loss: 0.1956\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1187 - val_loss: 0.1999\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1206 - val_loss: 0.1955\n",
            "\n",
            "Epoch 00137: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1125 - val_loss: 0.1923\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1086 - val_loss: 0.1918\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1109 - val_loss: 0.1930\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1097 - val_loss: 0.1923\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1098 - val_loss: 0.1958\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1107 - val_loss: 0.1929\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1091 - val_loss: 0.1969\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1112 - val_loss: 0.1915\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1091 - val_loss: 0.1917\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 22s 376ms/step - loss: 0.1084 - val_loss: 0.1928\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1064 - val_loss: 0.2004\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1107 - val_loss: 0.1929\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1051 - val_loss: 0.1930\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1057 - val_loss: 0.1920\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1045 - val_loss: 0.1962\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.1071 - val_loss: 0.1950\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1056 - val_loss: 0.1920\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.1047 - val_loss: 0.1918\n",
            "\n",
            "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0991 - val_loss: 0.1900\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0976 - val_loss: 0.1903\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0956 - val_loss: 0.1924\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 22s 380ms/step - loss: 0.0976 - val_loss: 0.1907\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0970 - val_loss: 0.1924\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0952 - val_loss: 0.1924\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0971 - val_loss: 0.1936\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 22s 377ms/step - loss: 0.0959 - val_loss: 0.1928\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0935 - val_loss: 0.1928\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0961 - val_loss: 0.1942\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0948 - val_loss: 0.1904\n",
            "\n",
            "Epoch 00166: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0891 - val_loss: 0.1901\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0869 - val_loss: 0.1901\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0874 - val_loss: 0.1906\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0882 - val_loss: 0.1903\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 22s 379ms/step - loss: 0.0872 - val_loss: 0.1913\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0869 - val_loss: 0.1905\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 23s 391ms/step - loss: 0.0866 - val_loss: 0.1909\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0877 - val_loss: 0.1911\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0861 - val_loss: 0.1906\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0873 - val_loss: 0.1940\n",
            "\n",
            "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0852 - val_loss: 0.1895\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0812 - val_loss: 0.1899\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0812 - val_loss: 0.1899\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0805 - val_loss: 0.1901\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0799 - val_loss: 0.1896\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 22s 378ms/step - loss: 0.0803 - val_loss: 0.1909\n",
            "Epoch 183/300\n",
            "25/59 [===========>..................] - ETA: 11s - loss: 0.0812"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m73VwznGBh5R",
        "outputId": "9f0a8850-f388-49b6-9ce5-263a25df5b63"
      },
      "source": [
        "print(\"mae:\", oof.eval(\"abs(pressure - pred)\").mean())\n",
        "print(\"mae inspiratory:\", oof.query(\"u_out == 0\").eval(\"abs(pressure - pred)\").mean())\n",
        "print(\"mae expiratory :\", oof.query(\"u_out == 1\").eval(\"abs(pressure - pred)\").mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae: 0.1700748569735916\n",
            "mae inspiratory: 0.18324426831174476\n",
            "mae expiratory : 0.16202780215683016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUZhpUwnmkg0"
      },
      "source": [
        "lstm_v6\n",
        "- mae: 0.1700748569735916\n",
        "- mae inspiratory: 0.18324426831174476\n",
        "- mae expiratory : 0.16202780215683016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ5xAxLlAjOu"
      },
      "source": [
        "lstm_v5\n",
        "- mae: 0.19870362716674497\n",
        "- mae inspiratory: 0.24693164942887788\n",
        "- mae expiratory : 0.169200847028661"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG7AXnOa3VXC"
      },
      "source": [
        "lstm_v4\n",
        "- mae: 0.19910630745529212\n",
        "- mae inspiratory: 0.24139623415898287\n",
        "- mae expiratory : 0.1732360695506279"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZm2Ebs5_0x"
      },
      "source": [
        "lstm_v3\n",
        "- mae: 0.1894350191416654\n",
        "- mae inspiratory: 0.22527142074094628\n",
        "- mae expiratory : 0.16751262988059495"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URMkZHrri_1y"
      },
      "source": [
        "lstm_v2\n",
        "- mae: 0.20938487189579605\n",
        "- mae inspiratory: 0.2124941942660988\n",
        "- mae expiratory : 0.19958701148249222"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQNEX6sk4lN"
      },
      "source": [
        "lstm_v1\n",
        "- mae: 0.18086928169783648\n",
        "- mae inpiratory: 0.2031012730817264\n",
        "- mae expiratory: 0.16726918940573152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo8ooYNAQsCy"
      },
      "source": [
        "***\n",
        "## sub generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSGjBh2qQraV"
      },
      "source": [
        "all_preds = list()\n",
        "test_tf = tf.convert_to_tensor(test, dtype=tf.float32)\n",
        "\n",
        "for model in models_by_fold:\n",
        "    preds = model.call(test_tf, training=False).numpy().squeeze().ravel()\n",
        "    all_preds.append(preds)\n",
        "\n",
        "test_raw[\"pressure\"] = np.mean(all_preds, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpBQRKIXi7Qo"
      },
      "source": [
        "# saves final sub\n",
        "sub = pd.merge(submission[\"id\"], test_raw[[\"id\",\"pressure\"]], how=\"left\", on=\"id\")\n",
        "sub[\"pressure\"] = sub[\"pressure\"].fillna(0)\n",
        "sub.to_csv(f\"{subs_path}/sub_lstm.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gmgjMIfjAT"
      },
      "source": [
        "# saves oof preds\n",
        "oof.to_csv(f\"{results_path}/oof_lstm.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo6W9e1QxlOV"
      },
      "source": [
        "# saves each model preds (pseudo labels)\n",
        "for fold,preds in enumerate(all_preds):\n",
        "    _sub = submission.copy()\n",
        "    _test = test_raw[[\"id\",\"pressure\"]].copy()\n",
        "    _test[\"pressure\"] = preds\n",
        "    _sub = pd.merge(_sub[\"id\"], _test[[\"id\",\"pressure\"]], how=\"left\", on=\"id\")\n",
        "    _sub[\"pressure\"] = _sub[\"pressure\"].fillna(0)\n",
        "    _sub.to_csv(f\"{results_path}/plabels_lstm_{fold}.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-EK2rCThxOr"
      },
      "source": [
        "***"
      ]
    }
  ]
}